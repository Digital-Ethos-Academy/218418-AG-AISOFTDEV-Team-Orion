{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the source code for our main application from `app/main.py`. Providing the full code as context is essential for the LLM to generate accurate and relevant tests.\n",
    "\n",
    "**Model Selection:**\n",
    "For generating tests, models with strong code understanding and logical reasoning are best. `gpt-4.1`, `o3`, `codex-mini`, and `gemini-2.5-pro` are all excellent choices for this task.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated test files.\n",
    "- `clean_llm_output()`: To clean up the generated Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body).\n",
    "4.  Generate another test for the `GET /users/` endpoint.\n",
    "5.  Save the generated tests into a file named `tests/test_main_simple.py`.\n",
    "\n",
    "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "# tests/test_main_simple.py\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from main import app, Base, engine, SessionLocal\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "\n",
      "# Initialize TestClient\n",
      "client = TestClient(app)\n",
      "\n",
      "# Create a new database session for testing\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "\n",
      "# Drop and recreate the tables for a fresh start before each test\n",
      "@pytest.fixture(autouse=True)\n",
      "def setup_and_teardown_db():\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    yield\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "def test_create_user():\n",
      "    # Define the user data to send in the request\n",
      "    user_data = {\n",
      "        \"username\": \"testuser\",\n",
      "        \"email\": \"testuser@example.com\",\n",
      "        \"full_name\": \"Test User\",\n",
      "        \"hashed_password\": \"hashedpassword\"\n",
      "    }\n",
      "    \n",
      "    # Send a POST request to create a new user\n",
      "    response = client.post(\"/users\", json=user_data)\n",
      "    \n",
      "    # Assert that the response status code is 201 Created\n",
      "    assert response.status_code == 201\n",
      "    \n",
      "    # Assert that the response body contains the correct user data\n",
      "    response_data = response.json()\n",
      "    assert response_data[\"username\"] == user_data[\"username\"]\n",
      "    assert response_data[\"email\"] == user_data[\"email\"]\n",
      "    assert response_data[\"full_name\"] == user_data[\"full_name\"]\n",
      "    assert \"id\" in response_data  # Ensure the 'id' is in the response\n",
      "\n",
      "def test_list_users():\n",
      "    # First, create a user to ensure the endpoint returns something\n",
      "    test_create_user()\n",
      "    \n",
      "    # Send a GET request to retrieve the list of users\n",
      "    response = client.get(\"/users\")\n",
      "    \n",
      "    # Assert that the response status code is 200 OK\n",
      "    assert response.status_code == 200\n",
      "    \n",
      "    # Assert that the response body is a list\n",
      "    response_data = response.json()\n",
      "    assert isinstance(response_data, list)\n",
      "    \n",
      "    # Assert that the list contains at least one user\n",
      "    assert len(response_data) > 0\n",
      "    assert response_data[0][\"username\"] == \"testuser\"\n",
      "✅ Successfully saved artifact to: tests/test_main_simple.py\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate happy path tests for your API.\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "act as a QA engineer, using the app_code as context, generate a pytest test function for the POST /users/ endpoint,\n",
    "asserting that a user is created successfully (e.g., checking for a 201 created or 200 OK status code and verifying the\n",
    "response body) generate another test for the GET /users/ endpoint save the genereated tests into a file named tests/test_main_simple.py\n",
    "use {app_code} as context for the tests\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Task:** Prompt the LLM to generate tests for common edge cases, such as providing invalid data or requesting a non-existent resource.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to write two new test functions:\n",
    "    * A test for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "    * A test for the `GET /users/{user_id}` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "**Expected Quality:** Two new `pytest` functions that verify the application handles common error scenarios correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "Next, create a test file, say `test_main.py`, and add the following test functions:\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate edge case tests.\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "use the {app_code} as context, write two new test functions: a test for the POST /users/ endpoint that tries\n",
    "to create a user with an emails that already exists, asserting that the API returns a 400 Bad Request error,\n",
    "and another test for GET /users/user_id andpoint that requests a non-existent user ID, asserting that the API\n",
    "returns a 404 Not Found error\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
    "\n",
    "> **Hint:** Why use an isolated database? Running tests against your actual development database can lead to data corruption and flaky, unreliable tests. A pytest fixture that creates a fresh, in-memory database for each test ensures that your tests are independent, repeatable, and have no side effects.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to generate a `pytest` fixture.\n",
    "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "4.  Crucially, it must override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
    "5.  Save the generated fixture code to a special file named `tests/conftest.py`.\n",
    "6.  Finally, create a new test file, `tests/test_main_with_fixture.py`, and ask the LLM to rewrite the happy-path tests from Challenge 1 to use the new database fixture.\n",
    "\n",
    "**Expected Quality:** Two new files, `tests/conftest.py` and `tests/test_main_with_fixture.py`, containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Pytest DB Fixture ---\n",
      "import pytest\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker, declarative_base\n",
      "from fastapi.testclient import TestClient\n",
      "from your_app_name.main import app  # Import your FastAPI app\n",
      "from your_app_name.database import Base, get_db  # Import your Base and get_db\n",
      "\n",
      "# Create a new SQLAlchemy engine instance\n",
      "SQLALCHEMY_DATABASE_URL = \"sqlite:///:memory:\"\n",
      "engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False})\n",
      "\n",
      "# Create a configured \"Session\" class\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "\n",
      "# Create a new base class for declarative class definitions\n",
      "Base = declarative_base()\n",
      "\n",
      "# Define your models here or import them\n",
      "# Example:\n",
      "# class User(Base):\n",
      "#     __tablename__ = 'users'\n",
      "#     id = Column(Integer, primary_key=True, index=True)\n",
      "#     name = Column(String, index=True)\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def db_session():\n",
      "    # Create the database tables\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    # Create a new database session\n",
      "    db = TestingSessionLocal()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "        # Drop the database tables\n",
      "        Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def client(db_session):\n",
      "    def override_get_db():\n",
      "        try:\n",
      "            yield db_session\n",
      "        finally:\n",
      "            db_session.close()\n",
      "\n",
      "    # Override the dependency in the FastAPI app\n",
      "    app.dependency_overrides[get_db] = override_get_db\n",
      "    # Create a TestClient for the app with overridden dependencies\n",
      "    with TestClient(app) as c:\n",
      "        yield c\n",
      "✅ Successfully saved artifact to: tests/conftest.py\n",
      "\n",
      "--- Generating Refactored Tests ---\n",
      "# tests/conftest.py\n",
      "\n",
      "import pytest\n",
      "from myapp import create_app  # Import your Flask app factory\n",
      "from myapp.database import db as _db  # Import your database instance\n",
      "\n",
      "@pytest.fixture(scope='module')\n",
      "def app():\n",
      "    \"\"\"Create and configure a new app instance for each test.\"\"\"\n",
      "    app = create_app({'TESTING': True})\n",
      "\n",
      "    # Establish an application context before running the tests.\n",
      "    with app.app_context():\n",
      "        yield app\n",
      "\n",
      "@pytest.fixture(scope='module')\n",
      "def db(app):\n",
      "    \"\"\"Setup the database for tests.\"\"\"\n",
      "    _db.create_all()\n",
      "\n",
      "    # Insert any necessary setup data here\n",
      "\n",
      "    yield _db\n",
      "\n",
      "    # Tear down the database after tests\n",
      "    _db.drop_all()\n",
      "✅ Successfully saved artifact to: tests/test_main_with_fixture.py\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate the pytest fixture for an isolated test database.\n",
    "db_fixture_prompt = f\"\"\"\n",
    "generate a pytest fixture, it should configure a temporary, in-memory SQLite database using SQLAlchemy,\n",
    "it needs to create all the database tables before the test runs and tear them down afterward, crucially,\n",
    "it must override the get_db dependency in your FastAPT app to use this temporary database during tests, save the\n",
    "generated fixture code to a special file named tests/conftest.py\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# TODO: Write a prompt to refactor the happy path tests to use the new fixture.\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "finally, create a new test file tests/test_main_with_fixture.py\n",
    "and rewrite the happy-path tests from Challenge 1 to use the new database fixture\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
