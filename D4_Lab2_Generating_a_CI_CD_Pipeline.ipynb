{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Estimated Time:** 75 minutes\n",
    "\n",
    "**Introduction:**\n",
    "A robust CI pipeline is the backbone of modern software development. It automatically builds and tests your code every time a change is made, catching bugs early and ensuring quality. In this lab, you will generate all the configuration-as-code artifacts needed to build a professional CI pipeline for our application.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load our application code to provide context for the LLM. The AI needs to see our code's imports to generate an accurate `requirements.txt` file.\n",
    "\n",
    "**Model Selection:**\n",
    "Models that are good at understanding code and structured data formats like YAML are ideal. `gpt-4.1`, `o3`, or `codex-mini` are strong choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated configuration files.\n",
    "- `clean_llm_output()`: To clean up the generated text and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Task:** Before we can build a Docker image, we need a list of our Python dependencies. Prompt the LLM to analyze your application code and generate a `requirements.txt` file.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt that provides the LLM with the source code of your FastAPI application (`app_code`).\n",
    "2.  Instruct it to analyze the `import` statements and generate a list of all external dependencies (like `fastapi`, `uvicorn`, `sqlalchemy`). You should also ask it to include `pytest` for testing.\n",
    "3.  The output should be formatted as a standard `requirements.txt` file.\n",
    "4.  Save the artifact to the project's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating requirements.txt ---\n",
      "To automate this process within your project, follow these steps:\n",
      "\n",
      "1. **Create the `requirements.txt`**: You can do this manually by creating a new file named `requirements.txt` in the root directory of your project.\n",
      "\n",
      "2. **Add the Dependencies**: Copy and paste the dependencies list into this file.\n",
      "\n",
      "3. **Save the File**: Make sure the file is saved in the root directory of your project.\n",
      "\n",
      "If you want to automate the extraction of dependencies from your code, consider using tools like `pipreqs`, which can generate a `requirements.txt` file based on the imports found in your project. However, be aware that additional manual adjustments might be necessary to refine the list to match your specific needs and versions.\n",
      "\n",
      "Here is how you could use `pipreqs`:\n",
      "✅ Successfully saved artifact to: requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate a requirements.txt file.\n",
    "requirements_prompt = f\"\"\"\n",
    "use app_code as context, analyze the import statements and generate a list of all\n",
    "external dependencies (like fasapi, uvicorn, sqlalchemy) include pytest for testing,\n",
    "the output should be formatted as a standard requirements.txt file, save the artifact\n",
    "to the project's root directory\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating requirements.txt ---\")\n",
    "if app_code:\n",
    "    requirements_content = get_completion(requirements_prompt, client, model_name, api_provider)\n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(cleaned_reqs)\n",
    "    save_artifact(cleaned_reqs, \"requirements.txt\")\n",
    "else:\n",
    "    print(\"Skipping requirements generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Task:** Generate a multi-stage `Dockerfile` to create an optimized and secure container image for our application.\n",
    "\n",
    "> **Tip:** Why a multi-stage Dockerfile? The first stage (the 'builder') installs all dependencies, including build-time tools. The final stage copies only the application code and the necessary installed packages. This results in a much smaller, more secure production image because it doesn't contain any unnecessary build tools.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt asking for a multi-stage `Dockerfile` for a Python FastAPI application.\n",
    "2.  Specify the following requirements:\n",
    "    * Use a slim Python base image (e.g., `python:3.11-slim`).\n",
    "    * The first stage should install dependencies from `requirements.txt`.\n",
    "    * The final stage should copy the installed dependencies and the application code.\n",
    "    * The `CMD` should execute the application using `uvicorn`.\n",
    "3.  Save the generated file as `Dockerfile` in the project's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Dockerfile ---\n",
      "# Stage 1: Install dependencies\n",
      "FROM python:3.11-slim AS builder\n",
      "\n",
      "# Set environment variables for Python\n",
      "ENV PYTHONDONTWRITEBYTECODE=1\n",
      "ENV PYTHONUNBUFFERED=1\n",
      "\n",
      "# Create and set the working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the requirements file\n",
      "COPY requirements.txt .\n",
      "\n",
      "# Install the dependencies\n",
      "RUN python -m pip install --upgrade pip && \\\n",
      "    pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Stage 2: Build the final image\n",
      "FROM python:3.11-slim\n",
      "\n",
      "# Set environment variables for Python\n",
      "ENV PYTHONDONTWRITEBYTECODE=1\n",
      "ENV PYTHONUNBUFFERED=1\n",
      "\n",
      "# Create and set the working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the dependencies from the builder stage\n",
      "COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages\n",
      "COPY --from=builder /usr/local/bin /usr/local/bin\n",
      "\n",
      "# Copy the application code\n",
      "COPY . .\n",
      "\n",
      "# Expose the port the app runs on\n",
      "EXPOSE 8000\n",
      "\n",
      "# Command to run the application\n",
      "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "✅ Successfully saved artifact to: Dockerfile\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate a multi-stage Dockerfile.\n",
    "dockerfile_prompt = \"\"\"\n",
    "make a multistage DockerFile for a Python FastAPI application, use a slim Python base image\n",
    "(e.g, python:3.11-slim), the first stage shuold copy the installed dependencies from\n",
    "requirements.txt. the final stage should copy the installed dependencies and the application\n",
    "code, the CMD should execute the application using uvicorn,save the generated file as DockerFile\n",
    "in the project's root\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Dockerfile ---\")\n",
    "dockerfile_content = get_completion(dockerfile_prompt, client, model_name, api_provider)\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "print(cleaned_dockerfile)\n",
    "\n",
    "if cleaned_dockerfile:\n",
    "    save_artifact(cleaned_dockerfile, \"Dockerfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Task:** Generate a complete GitHub Actions workflow file to automate the build and test process.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt to generate a GitHub Actions workflow file named `ci.yml`.\n",
    "2.  Specify the following requirements for the workflow:\n",
    "    * It should trigger on any `push` to the `main` branch.\n",
    "    * It should define a single job named `build-and-test` that runs on `ubuntu-latest`.\n",
    "    * The job should have steps to: 1) Check out the code, 2) Set up a Python environment, 3) Install dependencies from `requirements.txt`, and 4) Run the test suite using `pytest`.\n",
    "3.  Save the generated YAML file to `.github/workflows/ci.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating GitHub Actions Workflow ---\n",
      "name: CI\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches:\n",
      "      - main\n",
      "\n",
      "jobs:\n",
      "  build-and-test:\n",
      "    runs-on: ubuntu-latest\n",
      "\n",
      "    steps:\n",
      "      - name: Check out code\n",
      "        uses: actions/checkout@v2\n",
      "\n",
      "      - name: Set up Python\n",
      "        uses: actions/setup-python@v4\n",
      "        with:\n",
      "          python-version: '3.x'  # Specify the Python version you need\n",
      "\n",
      "      - name: Install dependencies\n",
      "        run: |\n",
      "          python -m pip install --upgrade pip\n",
      "          pip install -r requirements.txt\n",
      "\n",
      "      - name: Run tests\n",
      "        run: |\n",
      "          pytest\n",
      "✅ Successfully saved artifact to: .github/workflows/ci.yml\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate the GitHub Actions ci.yml file.\n",
    "ci_workflow_prompt = \"\"\"\n",
    "generate a Github Actions workflow files named ci.yml, it should trigger on any\n",
    "push to the main branch, it should define a single job named build-and-test that\n",
    "runs on ubuntu-latest, the job should have steps to 1) check out the code, 2) set up a Python\n",
    "environment, 3) install dependencies from the requirements.txt, and 4) run the test suite using\n",
    "pytest, save the generated YAML file to .github/workflows/ci.yml\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating GitHub Actions Workflow ---\")\n",
    "ci_workflow_content = get_completion(ci_workflow_prompt, client, model_name, api_provider)\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(cleaned_yaml)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    # Note: The save_artifact helper creates directories as needed.\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have now generated a complete, professional Continuous Integration pipeline using AI. You created the dependency list, the containerization configuration, and the automation workflow, all from simple prompts. This is a powerful demonstration of how AI can automate complex DevOps tasks, allowing teams to build and ship software with greater speed and confidence.\n",
    "\n",
    "> **Key Takeaway:** AI is a powerful tool for generating 'Configuration as Code' artifacts. By prompting for specific formats like `requirements.txt`, `Dockerfile`, or `ci.yml`, you can automate the creation of the files that define your entire build, test, and deployment processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
